# Анализ данных о продажах и остатках для оптимизации распределения товаров в розничной сети на примере 1 квартала 2024-2025 гг.

Бизнес-проблема: Низкая эффективность ручного планирования распределения товаров, ведущая к потерям продаж из-за аут-оф-стока (OOS) и замораживанию оборотных средств в излишках (overstock), особенно в волатильный праздничный период (1 квартал).

Аналитический запрос: На основе исторических данных за 1 кв. 2024 и 2025 гг. разработать аналитический инструмент (набор отчётов и правил), который позволит более обоснованно планировать распределение товаров на 1 кв. 2026 года.

Цели:
1.	Диагностика: Выявить товары и магазины с проблемами OOS (дефицит) и overstock (излишки).
2.	Анализ сезонности: Количественно оценить влияние праздников (14.02, 23.02, 08.03) на продажи соответствующих товаров.
3.	Оценка оборачиваемости: Рассчитать оборачиваемость товаров и выделить "мёртвые" (медленно оборачиваемые) позиции.
4.	Формирование рекомендаций: Создать конкретные рекомендации по планированию распределения на 2026 год, включая приоритеты для разных групп товаров и магазинов.

## Этапы кода:

### Блок 1: инициализация и подготовка среды - подготовка рабочего пространства: импорт библиотек, отключение предупреждений, настройка путей.

*Подблок 1.1.: Импорт библиотек*

  - import pandas as pd: Импорт библиотеки для работы с табличными данными (DataFrame).
  - import os: Импорт библиотеки для взаимодействия с операционной системой (работа с файлами и путями).
  - import warnings: Импорт библиотеки для управления предупреждениями.
  - warnings.filterwarnings('ignore'): Полное отключение вывода предупреждений для чистоты консоли.

### Блок 2: загрузка и консолидация данных - поиск исходных файлов, их чтение, проверка целостности и объединение в единый датасет.

*Подблок 2.1.: Определение пути к папке с данными*

*Подблок 2.2.: Получен список всех .xlsx файлов и отсортирован*

*Подблок 2.3.: Создание пустого списка list_of_dataframes для хранения загруженных данных*

*Подблок 2.4.: В цикле каждый файл прочитан в df_temp с помощью pd.read_excel, добавлена колонка Имя_файла (важно для отслеживания источника данных), и результат добавлен в список*

*Подблок 2.5.: Проверено, что все файлы имеют одинаковый набор колонок*

*Подблок 2.6.: С помощью pd.concat все DataFrame'ы объединены в один df_combined*

Как устроен процесс и почему выбран такой метод:

  - Автоматизация: Вместо ручного чтения 26 файлов используется цикл. Это исключает ошибки, экономит время и делает код масштабируемым (при добавлении новых недель код менять не нужно).
  - Контроль качества: Проверка column_sets — критически важный шаг. Он гарантирует, что структура данных не изменилась в течение года, и конкатенация пройдет корректно.
  - Информативность: Вывод в stdout названий файлов и количества строк в каждом из них — это форма логирования, позволяющая убедиться, что все файлы загружены без ошибок и имеют ожидаемый объем.

Выводы по этапу:

  - Инсайт 1: Успешно загружено 26 файлов, общее количество записей — 1 922 648.
  - Инсайт 2: Данные полностью консистентны (все 19 колонок совпадают), что говорит о хорошем качестве первичной отчетности.

Результат: Создан «сырой» объединенный датасет, готовый к первичному анализу.

### Блок 3: первичный анализ данных - быстрая оценка загруженного датасета без глубоких преобразований. Вывод ключевых метрик и проверка качества.

Что сделано: Проведена быстрая разведка объединенного датасета: оценен размер, память, уникальные значения, пропуски и базовая статистика.

*Подблок 3.1.: Краткий анализ данных (размер данных, объем памяти, кол-во уникальных магазинов, кол-во уникальных артикулов, период, полнота данных по годам, пропущенных значения, статистика по числовым колонкам, примеры данных)*

Выводы по этапу:

  - Инсайт 1 (Проблема качества данных): Огромный процент пропусков в колонках «Заказ сделан, шт» (93.6%) и «Продажи, шт» (70.7%). NaN здесь — это отсутствие события, т.е. 0.
  - Инсайт 2 (Проблема качества данных): Категориальные колонки «Блок отгрузки» и «Тип сезона» также содержат массу пропусков (78.4% и 53.3% соответственно). NaN здесь — это отсутствие признака (товар без блока, без сезона).
  - Инсайт 3 (Бизнес-метрики): Средние продажи в неделю составляют всего 2.92 шт., средние остатки — 12.65 шт. Это говорит о том, что большинство товаров продается медленно, а затоваривание склада магазина — потенциальная проблема.

### Блок 4: сохранение промежуточных результатов - сохранение "сырых" объединенных данных.

Что сделано: Сохранение объединенного "сырого" датасета в CSV и вывод краткого отчета об итогах загрузки.

*Подблок 4.1.: Сохранение данных - создание директории, экспорт в CSV*

*Подблок 4.2.: Краткий отчет*

Выводы по этапу:

  - Этап загрузки и верификации успешно завершен. Данные сохранены, их структура и объемы понятны. Код готов к этапу очистки.

### Блок 5: очистка и трансофрмация данных - основной этап подготовки данных к анализу. Оптимизация памяти, обработка пропусков, создание новых признаков.

*Подблок 5.1.: Очистка и преобразование данных (df = df_combined.copy(): Создание независимой копии данных для дальнейшей работы, чтобы не изменять оригинал.)*

*Подблок 5.2.: Обработка пропущенных значений (Числовые колонки: NaN → 0, Категориальные колонки: NaN → специальное значение, Проверка на наличие пропусков)*

*Подблок 5.3.: Создание новых признаков (Уникальный ключ для каждой записи)*

*Подблок 5.4.: Анализ данных после очистки (Статистика по новым признакам, Проверка уникальности, Проверка аномальных значений, Проверка дубликатов по уникальному ключу)*

Выводы по этапу:

  - Инсайт 1: Данные успешно подготовлены к анализу. Все пропуски обработаны, дубликаты будут найдены далее.
  - Инсайт 2: Только 29.3% записей — это продажи. 98.1% записей — это остатки. Это количественное подтверждение бизнес-проблемы: склад магазина забит товарами, которые плохо продаются.
  - Инсайт 3: 99.9% записей — активные. Это отличный показатель: почти по всем комбинациям "магазин-товар" были хоть какие-то движения (продажи, отгрузки или остатки). «Мертвых душ» практически нет.

### Блок 6: анализ дубликатов и их удаление - выявление полностью или частично дублирующихся строк и их устранение.

Что сделано: Поиск дубликатов по созданному `Уникальный_ключ`, анализ причин дублирования, удаление дубликатов.

1. `df['Уникальный_ключ'].duplicated(keep=False)`: Поиск всех вхождений дублирующихся ключей (помечает и первый, и повторы).
2. Анализ примеров дубликатов: Вывод различающихся колонок внутри групп дубликатов. Это важно для понимания природы ошибки (полное копирование или частичное изменение данных).
3. `df.drop_duplicates(subset=['Уникальный_ключ'], keep='first')`: Удаление строк с повторяющимся ключом, оставляя только первую запись.
4. Подсчет количества удаленных строк и процента от общего объема.

*Подблок 6.1. Анализ и удаление дубликатов*

Выводы по этапу:

  - Инсайт: Найдено 71 дублирующаяся запись.

Результат: Количество записей сократилось на 71 (до 1 922 577). Данные стали чище. Дальнейший анализ будет проводиться на уникальных сочетаниях «неделя-магазин-товар».

### Блок 7: расширенный анализ данных после очистки - детальная статистика по очищенному датасету

Что сделано: Анализ распределения по годам, анализ сезонности, анализ категорий магазинов, анализ кластеров.

*Подблок 7.1.: Проверка данных полсе удаления дубликатов*

*Подблок 7.2.: Дополнительный анализ*

Как устроен процесс и почему выбран такой метод:

  - После очистки можно доверять агрегациям.
  - Года: Простое value_counts(), чтобы увидеть динамику.
  - Категории и Кластеры: Базовый, но необходимый срез данных. Позволяет увидеть, какие форматы магазинов доминируют в выборке.

Выводы по этапу:

  - Инсайт 1: Количество записей в 2025 году выросло на ~20% по сравнению с 2024. Это может быть связано с ростом бизнеса или расширением ассортимента.

### Блок 8: сохранение очищенных данных - фиксация результатов этапа очистки.

Что сделано: Фиксация результатов. Сохранены 3 файла: `cleaned_data_final.csv` (все записи), `active_data_final.csv` (только активные) и `final_data_statistics.txt` (текстовый отчет).

*Подблок 8.1.: Сохранение окончательных данных (Сохранение полных очищенных данных (без дубликатов), сохранение только активных записей, сохранение сводной статистики)*

*Подблок 8.2.: Заключительный отчет*

Выводы по этапу: Этап подготовки данных (Data Preparation) полностью завершен. Данные готовы к исследовательскому анализу.

### Блок 9: исследовательский анализ данных (EDA) - глубокий анализ бизнес-показателей на основе активных данных. Выявление проблем и классификация товаров.

Что сделано: Анализ OOS (Out of Stock), Overstock, сезонности и ABC-XYZ классификация.

*Подблок 9.1.: Анализ OOS (Дефицит)*

  - Логика: Ситуация риска дефицита определяется как Остатки < Продажи. Создание флага Остатки_малы.
  - Товарный разрез: Поиск товаров, которые чаще всего попадают в ситуацию дефицита (топ-10).
  - Магазинный разрез: Поиск магазинов с наибольшим числом таких ситуаций.

*Подблок 9.2.: Анализ Overstock (Излишки)*

  - Метрика: Дней_продаж_в_остатке = (Ср. остатки / Ср. недельные продажи) * 7. Сколько дней торговли "заморожено" на складе.
  - Порог: > 60 дней — критический overstock.
  - Выявление товаров и магазинов с максимальным значением этой метрики.

*Подблок 9.3.: Анализ сезонности*

  - Цель: Оценка влияния праздников (14 февраля, 23 февраля, 8 марта) на продажи.
  - Метод: Сравнение средних продаж в "праздничные" недели (7-11) с обычными неделями.
  - Результат: Коэффициент сезонности (рост продаж в ~1.4 раза).

*Подблок 9.4.: ABC-XYZ анализ для классификации товаров*

  - ABC (значимость): Сортировка товаров по сумме продаж. Правило Парето.
    - A: 80% выручки (топ товары).
    - B: Следующие 15%.
    - C: Последние 5% (аутсайдеры).
  - XYZ (стабильность спроса): Расчет коэффициента вариации продаж по неделям.
    - X: Стабильный спрос (CV < 0.3).
    - Y: Средний (0.3 < CV < 0.7).
    - Z: Нестабильный/хаотичный (CV > 0.7).
  - Матрица ABC-XYZ: Комбинация групп. Ключевые выводы:
    - AX: Золотой фонд (важно и стабильно).
    - CZ: Кандидаты на вывод из ассортимента (мало продаются и непредсказуемы).
   
*Подблок 9.5.: Итоги EDA*

Выводы по этапу:

  - OOS: 5 445 товаров имеют риск дефицита. Лидер антирейтинга — «Сахарные соты...» (133 недели риска). Москва — зона повышенного риска.
  - Overstock: После корректировки выявлено 735 товаров с запасами более чем на 2 месяца вперед. Некоторые товары (подарочная упаковка) имеют запасы на год вперед.
  - Сезонность: Пик продаж — неделя 7 (2025). Продажи в праздничные недели выше в 1.4 раза.
  - ABC-XYZ: Группа A (80% продаж) — это всего 1 429 товаров (26%). Группа CZ (неликвидный сток) — 2 600 товаров (48%). Это главный бизнес-вывод.

### Блок 10: визиуализация данных - построение графиков для наглядного представления результатов анализа.

### Блок 11: прогнозирование и планирование (нормы отгрузок на 1 квартал 2026 года) - финальный этап — формирование готового управленческого отчета и плана закупок на следующий период.

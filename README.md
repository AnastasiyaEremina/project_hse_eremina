# Анализ данных о продажах и остатках для оптимизации распределения товаров в розничной сети на примере 1 квартала 2024-2025 гг.

Бизнес-проблема: Низкая эффективность ручного планирования распределения товаров, ведущая к потерям продаж из-за аут-оф-стока (OOS) и замораживанию оборотных средств в излишках (overstock), особенно в волатильный праздничный период (1 квартал).

Аналитический запрос: На основе исторических данных за 1 кв. 2024 и 2025 гг. разработать аналитический инструмент (набор отчётов и правил), который позволит более обоснованно планировать распределение товаров на 1 кв. 2026 года.

Цели:
1.	Диагностика: Выявить товары и магазины с проблемами OOS (дефицит) и overstock (излишки).
2.	Анализ сезонности: Количественно оценить влияние праздников (14.02, 23.02, 08.03) на продажи соответствующих товаров.
3.	Оценка оборачиваемости: Рассчитать оборачиваемость товаров и выделить "мёртвые" (медленно оборачиваемые) позиции.
4.	Формирование рекомендаций: Создать конкретные рекомендации по планированию распределения на 2026 год, включая приоритеты для разных групп товаров и магазинов.

## Этапы кода:

### Блок 1: инициализация и подготовка среды - подготовка рабочего пространства: импорт библиотек, отключение предупреждений, настройка путей.

*Подблок 1.1.: Импорт библиотек*

  - import pandas as pd: Импорт библиотеки для работы с табличными данными (DataFrame).
  - import os: Импорт библиотеки для взаимодействия с операционной системой (работа с файлами и путями).
  - import warnings: Импорт библиотеки для управления предупреждениями.
  - warnings.filterwarnings('ignore'): Полное отключение вывода предупреждений для чистоты консоли.

### Блок 2: загрузка и консолидация данных - поиск исходных файлов, их чтение, проверка целостности и объединение в единый датасет.

*Подблок 2.1.: Определение пути к папке с данными*

*Подблок 2.2.: Получен список всех .xlsx файлов и отсортирован*

*Подблок 2.3.: Создание пустого списка list_of_dataframes для хранения загруженных данных*

*Подблок 2.4.: В цикле каждый файл прочитан в df_temp с помощью pd.read_excel, добавлена колонка Имя_файла (важно для отслеживания источника данных), и результат добавлен в список*

*Подблок 2.5.: Проверено, что все файлы имеют одинаковый набор колонок*

*Подблок 2.6.: С помощью pd.concat все DataFrame'ы объединены в один df_combined*

Как устроен процесс и почему выбран такой метод:

  - Автоматизация: Вместо ручного чтения 26 файлов используется цикл. Это исключает ошибки, экономит время и делает код масштабируемым (при добавлении новых недель код менять не нужно).
  - Контроль качества: Проверка column_sets — критически важный шаг. Он гарантирует, что структура данных не изменилась в течение года, и конкатенация пройдет корректно.
  - Информативность: Вывод в stdout названий файлов и количества строк в каждом из них — это форма логирования, позволяющая убедиться, что все файлы загружены без ошибок и имеют ожидаемый объем.

Выводы по этапу:

  - Инсайт 1: Успешно загружено 26 файлов, общее количество записей — 1 922 648.
  - Инсайт 2: Данные полностью консистентны (все 19 колонок совпадают), что говорит о хорошем качестве первичной отчетности.

Результат: Создан «сырой» объединенный датасет, готовый к первичному анализу.

### Блок 3: первичный анализ данных - быстрая оценка загруженного датасета без глубоких преобразований. Вывод ключевых метрик и проверка качества.

Что сделано: Проведена быстрая разведка объединенного датасета: оценен размер, память, уникальные значения, пропуски и базовая статистика.

*Подблок 3.1.: Краткий анализ данных (размер данных, объем памяти, кол-во уникальных магазинов, кол-во уникальных артикулов, период, полнота данных по годам, пропущенных значения, статистика по числовым колонкам, примеры данных)*

Выводы по этапу:

  - Инсайт 1 (Проблема качества данных): Огромный процент пропусков в колонках «Заказ сделан, шт» (93.6%) и «Продажи, шт» (70.7%). NaN здесь — это отсутствие события, т.е. 0.
  - Инсайт 2 (Проблема качества данных): Категориальные колонки «Блок отгрузки» и «Тип сезона» также содержат массу пропусков (78.4% и 53.3% соответственно). NaN здесь — это отсутствие признака (товар без блока, без сезона).
  - Инсайт 3 (Бизнес-метрики): Средние продажи в неделю составляют всего 2.92 шт., средние остатки — 12.65 шт. Это говорит о том, что большинство товаров продается медленно, а затоваривание склада магазина — потенциальная проблема.

### Блок 4: сохранение промежуточных результатов - сохранение "сырых" объединенных данных.

Что сделано: Сохранение объединенного "сырого" датасета в CSV и вывод краткого отчета об итогах загрузки.

*Подблок 4.1.: Сохранение данных - создание директории, экспорт в CSV*

*Подблок 4.2.: Краткий отчет*

Выводы по этапу:

  - Этап загрузки и верификации успешно завершен. Данные сохранены, их структура и объемы понятны. Код готов к этапу очистки.

### Блок 5: очистка и трансофрмация данных - основной этап подготовки данных к анализу. Оптимизация памяти, обработка пропусков, создание новых признаков.

*Подблок 5.1.: Очистка и преобразование данных (df = df_combined.copy(): Создание независимой копии данных для дальнейшей работы, чтобы не изменять оригинал.)*

*Подблок 5.2.: Обработка пропущенных значений (Числовые колонки: NaN → 0, Категориальные колонки: NaN → специальное значение, Проверка на наличие пропусков)*

*Подблок 5.3.: Создание новых признаков (Уникальный ключ для каждой записи)*

*Подблок 5.4.: Анализ данных после очистки (Статистика по новым признакам, Проверка уникальности, Проверка аномальных значений, Проверка дубликатов по уникальному ключу)*

Выводы по этапу:

  - Инсайт 1: Данные успешно подготовлены к анализу. Все пропуски обработаны, дубликаты будут найдены далее.
  - Инсайт 2: Только 29.3% записей — это продажи. 98.1% записей — это остатки. Это количественное подтверждение бизнес-проблемы: склад магазина забит товарами, которые плохо продаются.
  - Инсайт 3: 99.9% записей — активные. Это отличный показатель: почти по всем комбинациям "магазин-товар" были хоть какие-то движения (продажи, отгрузки или остатки). «Мертвых душ» практически нет.

### Блок 6: анализ дубликатов и их удаление - выявление полностью или частично дублирующихся строк и их устранение.

Что сделано: Поиск дубликатов по созданному `Уникальный_ключ`, анализ причин дублирования, удаление дубликатов.

1. `df['Уникальный_ключ'].duplicated(keep=False)`: Поиск всех вхождений дублирующихся ключей (помечает и первый, и повторы).
2. Анализ примеров дубликатов: Вывод различающихся колонок внутри групп дубликатов. Это важно для понимания природы ошибки (полное копирование или частичное изменение данных).
3. `df.drop_duplicates(subset=['Уникальный_ключ'], keep='first')`: Удаление строк с повторяющимся ключом, оставляя только первую запись.
4. Подсчет количества удаленных строк и процента от общего объема.

*Подблок 6.1. Анализ и удаление дубликатов*

Выводы по этапу:

  - Инсайт: Найдено 71 дублирующаяся запись.

Результат: Количество записей сократилось на 71 (до 1 922 577). Данные стали чище. Дальнейший анализ будет проводиться на уникальных сочетаниях «неделя-магазин-товар».

### Блок 7: расширенный анализ данных после очистки - детальная статистика по очищенному датасету

### Блок 8: сохранение очищенных данных - фиксация результатов этапа очистки.

### Блок 9: исследовательский анализ данных (EDA) - глубокий анализ бизнес-показателей на основе активных данных. Выявление проблем и классификация товаров.

### Блок 10: визиуализация данных - построение графиков для наглядного представления результатов анализа.

### Блок 11: прогнозирование и планирование (нормы отгрузок на 1 квартал 2026 года) - финальный этап — формирование готового управленческого отчета и плана закупок на следующий период.
